{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnli+2tBB+Q1DVX6sW1bR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atif923/LLM_Bounty/blob/main/LLM_Bounty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip -q install pyarrow==15.0.0 gradio datasets fuzzywuzzy"
      ],
      "metadata": {
        "id": "wai0KTbq5hYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mental Health Assistant Chatbot Implementation\n",
        "\n",
        "This code implements a mental health assistant chatbot using LLaMA for text generation and BERT for sentiment analysis. It processes user inputs to determine sentiment, retrieves relevant responses from a dataset, or generates original responses when no match is found. The Gradio interface allows users to interact with the chatbot, providing an intuitive platform to receive support and advice. The assistant adjusts its responses based on the sentiment detected, ensuring a compassionate and context-aware interaction.\n",
        "\n",
        "### About the Dataset\n",
        "The Amod/mental_health_counseling_conversations dataset [Health Counseling Dataset](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations), obtained from Hugging Face Datasets, comprises questions and answers from two online counseling platforms, covering various mental health topics. Responses are provided by qualified psychologists, making the dataset suitable for fine-tuning language models to enhance their ability to offer mental health advice. Each data instance includes a \"Context\" (the user's question) and a \"Response\" (the psychologist's answer). The dataset is in English, and while it may contain sensitive information, all data has been anonymized to ensure privacy. Users can create custom data splits as needed."
      ],
      "metadata": {
        "id": "Hn1UiUajjT1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,BertTokenizer, BertForSequenceClassification\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import gradio as gr\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from fuzzywuzzy import process"
      ],
      "metadata": {
        "id": "8d7K5Fwt8-ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to huggingface with the token\n",
        "# login(token='Replace with the huggingface login token')\n",
        "login(token='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')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DcZvGNM5j1gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for text generation\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1024,  # Adjust max_length as needed\n",
        ")"
      ],
      "metadata": {
        "id": "84ytwODck1xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for Sentiment Analysis of User Input\n",
        "sentiment_model_name = \"bert-base-uncased\"  # You can choose any supported sentiment analysis model\n",
        "sentiment_model = BertForSequenceClassification.from_pretrained(sentiment_model_name)\n",
        "sentiment_tokenizer = BertTokenizer.from_pretrained(sentiment_model_name)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_tokenizer,device_map=\"auto\")\n",
        "\n",
        "label_mapping = {'LABEL_0':'NEGATIVE','LABEL_1':'POSITIVE'}"
      ],
      "metadata": {
        "id": "8dpYMjU5ljHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response based on similar context from the mentioned dataset\n",
        "dataset = load_dataset('Amod/mental_health_counseling_conversations')\n",
        "context_column = 'Context'\n",
        "response_column = 'Response'"
      ],
      "metadata": {
        "id": "DiyOMAiBlMqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(sentiment_label,user_input = None, dataset_response = None):\n",
        "    \"\"\"Generate response based on sentiment.\"\"\"\n",
        "    if dataset_response:\n",
        "      response = dataset_response\n",
        "    else:\n",
        "      # Generate response with optimized parameters\n",
        "      response = llama_pipeline(user_input, max_length=1000, do_sample=True,truncation=True)[0]['generated_text']\n",
        "      response = response.replace(f\"Answer: {user_input}\", \"\").replace(f\"{user_input}\", \"\").strip()\n",
        "\n",
        "      # Adjust response based on sentiment\n",
        "      if sentiment_label == 'NEGATIVE':\n",
        "          response = f\"I'm really sorry you're feeling this way. Remember, it's okay to seek help and talk to someone you trust. Here's some advice that might help: {response}\"\n",
        "      elif sentiment_label == 'POSITIVE':\n",
        "          response = f\"That's great to hear! I'm glad you're feeling good. Keep up the positive mindset! Here's some more encouragement: {response}\"\n",
        "      elif sentiment_label == 'NEUTRAL':\n",
        "          response = f\"Thank you for sharing your thoughts. Here's something you might find interesting: {response}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "def find_similar_context(user_input):\n",
        "    \"\"\"Find the most similar context from the dataset and return the match score.\"\"\"\n",
        "    contexts = dataset['train'][context_column]  # Adjust if dataset splits are different\n",
        "    best_match = process.extractOne(user_input, contexts)\n",
        "    if best_match:\n",
        "        matched_context, score = best_match\n",
        "        return matched_context, score\n",
        "    else:\n",
        "        return None, 0  # Return a score of 0 if no match is found\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # Analyze sentiment\n",
        "    sentiment = sentiment_analyzer(user_input)[0]\n",
        "    sentiment_label = label_mapping.get(sentiment['label'], 'NEUTRAL')\n",
        "\n",
        "    # Search for similar context in the dataset\n",
        "    similar_context, match_score = find_similar_context(user_input)\n",
        "\n",
        "    if similar_context:\n",
        "        # Find the corresponding response from the dataset\n",
        "        context_index = dataset['train'][context_column].index(similar_context)\n",
        "        dataset_response = dataset['train'][response_column][context_index]\n",
        "\n",
        "        # Generate a response based on the dataset response and sentiment\n",
        "        response = generate_response(sentiment_label,dataset_response = dataset_response)\n",
        "    else:\n",
        "        # Generate a response using the LLaMA pipeline\n",
        "        response = generate_response(sentiment_label,user_input = user_input)\n",
        "\n",
        "    return response\n",
        "\n",
        "def respond(user_input):\n",
        "    output_message = \"Please wait, generating response...\"\n",
        "    gr.update(value=output_message)  # Update output to show waiting message\n",
        "    response = chatbot_response(user_input)  # Call the main response function\n",
        "    return response\n",
        "\n",
        "# Define and launch the Gradio interface\n",
        "# Define and launch the Gradio interface with a waiting message\n",
        "interface = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Mental Health Assistant\",\n",
        "    allow_flagging=\"never\",\n",
        "    description=\"Please enter your message below and wait for a response.\"\n",
        ")\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "U-J7GD0angDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}